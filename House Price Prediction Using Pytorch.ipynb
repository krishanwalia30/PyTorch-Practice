{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources to read about embedding in neural network**\n",
    "- https://docs.fast.ai/tabular.html \n",
    "- https://www.fast.ai/2018/04/29/categorical-embeddings/ \n",
    "- https://www.fast.ai/2018/04/29/categorical-embeddings/ \n",
    "- https://yashuseth.blog/2018/07/22/pytorch-neural-network-for-tabular-data-with-categorical-embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('houseprice.csv', usecols= [\"SalePrice\", \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n",
    "                                         \"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>2003</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2001</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1915</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  YearBuilt  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg       2003   \n",
       "1          20       RL         80.0     9600   Pave      Reg       1976   \n",
       "2          60       RL         68.0    11250   Pave      IR1       2001   \n",
       "3          70       RL         60.0     9550   Pave      IR1       1915   \n",
       "4          60       RL         84.0    14260   Pave      IR1       2000   \n",
       "\n",
       "   1stFlrSF  2ndFlrSF  SalePrice  \n",
       "0       856       854     208500  \n",
       "1      1262         0     181500  \n",
       "2       920       866     223500  \n",
       "3       961       756     140000  \n",
       "4      1145      1053     250000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1201 entries, 0 to 1459\n",
      "Data columns (total 10 columns):\n",
      "MSSubClass     1201 non-null int64\n",
      "MSZoning       1201 non-null object\n",
      "LotFrontage    1201 non-null float64\n",
      "LotArea        1201 non-null int64\n",
      "Street         1201 non-null object\n",
      "LotShape       1201 non-null object\n",
      "YearBuilt      1201 non-null int64\n",
      "1stFlrSF       1201 non-null int64\n",
      "2ndFlrSF       1201 non-null int64\n",
      "SalePrice      1201 non-null int64\n",
      "dtypes: float64(1), int64(6), object(3)\n",
      "memory usage: 103.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name: MSSubClass and the number of uniqure values: 15\n",
      "Column name: MSZoning and the number of uniqure values: 5\n",
      "Column name: LotFrontage and the number of uniqure values: 110\n",
      "Column name: LotArea and the number of uniqure values: 869\n",
      "Column name: Street and the number of uniqure values: 2\n",
      "Column name: LotShape and the number of uniqure values: 4\n",
      "Column name: YearBuilt and the number of uniqure values: 112\n",
      "Column name: 1stFlrSF and the number of uniqure values: 678\n",
      "Column name: 2ndFlrSF and the number of uniqure values: 368\n",
      "Column name: SalePrice and the number of uniqure values: 597\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(f\"Column name: {i} and the number of uniqure values: {df[i].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total Years'] = datetime.datetime.now().year - df['YearBuilt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"YearBuilt\",axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'LotShape', '1stFlrSF', '2ndFlrSF', 'SalePrice', 'Total Years'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['MSSubClass', 'MSZoning','Street','LotShape']\n",
    "out_feature = 'SalePrice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 5, ..., 6, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl_encoders = {}\n",
    "lbl_encoders['MSSubClass'] = LabelEncoder()\n",
    "lbl_encoders['MSSubClass'].fit_transform(df['MSSubClass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl_encoders = {}\n",
    "for feature in cat_features:\n",
    "    lbl_encoders[feature] = LabelEncoder()\n",
    "    df[feature] = lbl_encoders[feature].fit_transform(df[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Total Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>14115</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>796</td>\n",
       "      <td>566</td>\n",
       "      <td>143000</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10084</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1694</td>\n",
       "      <td>0</td>\n",
       "      <td>307000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1022</td>\n",
       "      <td>752</td>\n",
       "      <td>129900</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7420</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1077</td>\n",
       "      <td>0</td>\n",
       "      <td>118000</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11200</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1040</td>\n",
       "      <td>0</td>\n",
       "      <td>129500</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11924</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1182</td>\n",
       "      <td>1142</td>\n",
       "      <td>345000</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>91.0</td>\n",
       "      <td>10652</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1494</td>\n",
       "      <td>0</td>\n",
       "      <td>279500</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>132000</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10791</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "      <td>90000</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>66.0</td>\n",
       "      <td>13695</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1114</td>\n",
       "      <td>0</td>\n",
       "      <td>159000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7560</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1339</td>\n",
       "      <td>0</td>\n",
       "      <td>139000</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>101.0</td>\n",
       "      <td>14215</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1158</td>\n",
       "      <td>1218</td>\n",
       "      <td>325300</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7449</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1108</td>\n",
       "      <td>0</td>\n",
       "      <td>139400</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9742</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1795</td>\n",
       "      <td>0</td>\n",
       "      <td>230000</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4224</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>129900</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>110.0</td>\n",
       "      <td>14230</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>256300</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>7200</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>900</td>\n",
       "      <td>0</td>\n",
       "      <td>134800</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>98.0</td>\n",
       "      <td>11478</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1704</td>\n",
       "      <td>0</td>\n",
       "      <td>306000</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>16321</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>207500</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6324</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>0</td>\n",
       "      <td>68500</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>8500</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>649</td>\n",
       "      <td>668</td>\n",
       "      <td>40000</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11049</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1234</td>\n",
       "      <td>0</td>\n",
       "      <td>179900</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>10552</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1700</td>\n",
       "      <td>0</td>\n",
       "      <td>165500</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>7313</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1561</td>\n",
       "      <td>0</td>\n",
       "      <td>277500</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>108.0</td>\n",
       "      <td>13418</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1132</td>\n",
       "      <td>1320</td>\n",
       "      <td>309000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>10721</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1252</td>\n",
       "      <td>0</td>\n",
       "      <td>142000</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>81.0</td>\n",
       "      <td>10944</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1223</td>\n",
       "      <td>904</td>\n",
       "      <td>271000</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10930</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1048</td>\n",
       "      <td>510</td>\n",
       "      <td>140000</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>7200</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>804</td>\n",
       "      <td>0</td>\n",
       "      <td>119000</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>21930</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>734</td>\n",
       "      <td>1104</td>\n",
       "      <td>192140</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10800</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>968</td>\n",
       "      <td>0</td>\n",
       "      <td>64500</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>93.0</td>\n",
       "      <td>10261</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>962</td>\n",
       "      <td>830</td>\n",
       "      <td>186500</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>17400</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1126</td>\n",
       "      <td>0</td>\n",
       "      <td>160000</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8400</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1537</td>\n",
       "      <td>0</td>\n",
       "      <td>174000</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>864</td>\n",
       "      <td>0</td>\n",
       "      <td>120500</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>96.0</td>\n",
       "      <td>12444</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1932</td>\n",
       "      <td>0</td>\n",
       "      <td>394617</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7407</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1236</td>\n",
       "      <td>0</td>\n",
       "      <td>149700</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11584</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1040</td>\n",
       "      <td>685</td>\n",
       "      <td>197000</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>79.0</td>\n",
       "      <td>11526</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1423</td>\n",
       "      <td>748</td>\n",
       "      <td>191000</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11003</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1026</td>\n",
       "      <td>981</td>\n",
       "      <td>310000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>63.0</td>\n",
       "      <td>8500</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1422</td>\n",
       "      <td>0</td>\n",
       "      <td>179600</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8400</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>129000</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1220</td>\n",
       "      <td>870</td>\n",
       "      <td>240000</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11767</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>796</td>\n",
       "      <td>550</td>\n",
       "      <td>112000</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1533</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>630</td>\n",
       "      <td>0</td>\n",
       "      <td>92000</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>896</td>\n",
       "      <td>896</td>\n",
       "      <td>136000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9262</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1578</td>\n",
       "      <td>0</td>\n",
       "      <td>287090</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3675</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1072</td>\n",
       "      <td>0</td>\n",
       "      <td>145000</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>17217</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1140</td>\n",
       "      <td>0</td>\n",
       "      <td>84500</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7500</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1221</td>\n",
       "      <td>0</td>\n",
       "      <td>185000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>953</td>\n",
       "      <td>694</td>\n",
       "      <td>175000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2073</td>\n",
       "      <td>0</td>\n",
       "      <td>210000</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1188</td>\n",
       "      <td>1152</td>\n",
       "      <td>266500</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1078</td>\n",
       "      <td>0</td>\n",
       "      <td>142125</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1256</td>\n",
       "      <td>0</td>\n",
       "      <td>147500</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1201 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  1stFlrSF  \\\n",
       "0              5         3         65.0     8450       1         3       856   \n",
       "1              0         3         80.0     9600       1         3      1262   \n",
       "2              5         3         68.0    11250       1         0       920   \n",
       "3              6         3         60.0     9550       1         0       961   \n",
       "4              5         3         84.0    14260       1         0      1145   \n",
       "5              4         3         85.0    14115       1         0       796   \n",
       "6              0         3         75.0    10084       1         3      1694   \n",
       "8              4         4         51.0     6120       1         3      1022   \n",
       "9             14         3         50.0     7420       1         3      1077   \n",
       "10             0         3         70.0    11200       1         3      1040   \n",
       "11             5         3         85.0    11924       1         0      1182   \n",
       "13             0         3         91.0    10652       1         0      1494   \n",
       "15             3         4         51.0     6120       1         3       854   \n",
       "17            10         3         72.0    10791       1         3      1296   \n",
       "18             0         3         66.0    13695       1         3      1114   \n",
       "19             0         3         70.0     7560       1         3      1339   \n",
       "20             5         3        101.0    14215       1         0      1158   \n",
       "21             3         4         57.0     7449       1         3      1108   \n",
       "22             0         3         75.0     9742       1         3      1795   \n",
       "23            11         4         44.0     4224       1         3      1060   \n",
       "25             0         3        110.0    14230       1         3      1600   \n",
       "26             0         3         60.0     7200       1         3       900   \n",
       "27             0         3         98.0    11478       1         3      1704   \n",
       "28             0         3         47.0    16321       1         0      1600   \n",
       "29             1         4         60.0     6324       1         0       520   \n",
       "30             6         0         50.0     8500       1         3       649   \n",
       "32             0         3         85.0    11049       1         3      1234   \n",
       "33             0         3         70.0    10552       1         0      1700   \n",
       "34            11         3         60.0     7313       1         3      1561   \n",
       "35             5         3        108.0    13418       1         3      1132   \n",
       "...          ...       ...          ...      ...     ...       ...       ...   \n",
       "1425           0         3         80.0    10721       1         0      1252   \n",
       "1426           5         3         81.0    10944       1         0      1223   \n",
       "1427           4         3         60.0    10930       1         3      1048   \n",
       "1428           1         4         60.0     7200       1         3       804   \n",
       "1430           5         3         60.0    21930       1         2       734   \n",
       "1432           1         3         60.0    10800       1         3       968   \n",
       "1433           5         3         93.0    10261       1         0       962   \n",
       "1434           0         3         80.0    17400       1         3      1126   \n",
       "1435           0         3         80.0     8400       1         3      1537   \n",
       "1436           0         3         60.0     9000       1         3       864   \n",
       "1437           0         3         96.0    12444       1         3      1932   \n",
       "1438           0         4         90.0     7407       1         3      1236   \n",
       "1439           5         3         80.0    11584       1         3      1040   \n",
       "1440           6         3         79.0    11526       1         0      1423   \n",
       "1442           5         1         85.0    11003       1         3      1026   \n",
       "1444           0         3         63.0     8500       1         3      1422   \n",
       "1445           9         3         70.0     8400       1         3       913   \n",
       "1447           5         3         80.0    10000       1         3      1220   \n",
       "1448           4         3         70.0    11767       1         3       796   \n",
       "1449          13         4         21.0     1533       1         3       630   \n",
       "1450          10         3         60.0     9000       1         3       896   \n",
       "1451           0         3         78.0     9262       1         3      1578   \n",
       "1452          13         4         35.0     3675       1         3      1072   \n",
       "1453           0         3         90.0    17217       1         3      1140   \n",
       "1454           0         1         62.0     7500       1         3      1221   \n",
       "1455           5         3         62.0     7917       1         3       953   \n",
       "1456           0         3         85.0    13175       1         3      2073   \n",
       "1457           6         3         66.0     9042       1         3      1188   \n",
       "1458           0         3         68.0     9717       1         3      1078   \n",
       "1459           0         3         75.0     9937       1         3      1256   \n",
       "\n",
       "      2ndFlrSF  SalePrice  Total Years  \n",
       "0          854     208500           21  \n",
       "1            0     181500           48  \n",
       "2          866     223500           23  \n",
       "3          756     140000          109  \n",
       "4         1053     250000           24  \n",
       "5          566     143000           31  \n",
       "6            0     307000           20  \n",
       "8          752     129900           93  \n",
       "9            0     118000           85  \n",
       "10           0     129500           59  \n",
       "11        1142     345000           19  \n",
       "13           0     279500           18  \n",
       "15           0     132000           95  \n",
       "17           0      90000           57  \n",
       "18           0     159000           20  \n",
       "19           0     139000           66  \n",
       "20        1218     325300           19  \n",
       "21           0     139400           94  \n",
       "22           0     230000           22  \n",
       "23           0     129900           48  \n",
       "25           0     256300           17  \n",
       "26           0     134800           73  \n",
       "27           0     306000           17  \n",
       "28           0     207500           67  \n",
       "29           0      68500           97  \n",
       "30         668      40000          104  \n",
       "32           0     179900           17  \n",
       "33           0     165500           65  \n",
       "34           0     277500           19  \n",
       "35        1320     309000           20  \n",
       "...        ...        ...          ...  \n",
       "1425         0     142000           65  \n",
       "1426       904     271000           30  \n",
       "1427       510     140000           79  \n",
       "1428         0     119000           84  \n",
       "1430      1104     192140           19  \n",
       "1432         0      64500           97  \n",
       "1433       830     186500           24  \n",
       "1434         0     160000           47  \n",
       "1435         0     174000           62  \n",
       "1436         0     120500           53  \n",
       "1437         0     394617           16  \n",
       "1438         0     149700           67  \n",
       "1439       685     197000           45  \n",
       "1440       748     191000          102  \n",
       "1442       981     310000           16  \n",
       "1444         0     179600           20  \n",
       "1445         0     129000           58  \n",
       "1447       870     240000           29  \n",
       "1448       550     112000          114  \n",
       "1449         0      92000           54  \n",
       "1450       896     136000           50  \n",
       "1451         0     287090           16  \n",
       "1452         0     145000           19  \n",
       "1453         0      84500           18  \n",
       "1454         0     185000           20  \n",
       "1455       694     175000           25  \n",
       "1456         0     210000           46  \n",
       "1457      1152     266500           83  \n",
       "1458         0     142125           74  \n",
       "1459         0     147500           59  \n",
       "\n",
       "[1201 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1201 entries, 0 to 1459\n",
      "Data columns (total 10 columns):\n",
      "MSSubClass     1201 non-null int64\n",
      "MSZoning       1201 non-null int32\n",
      "LotFrontage    1201 non-null float64\n",
      "LotArea        1201 non-null int64\n",
      "Street         1201 non-null int32\n",
      "LotShape       1201 non-null int32\n",
      "1stFlrSF       1201 non-null int64\n",
      "2ndFlrSF       1201 non-null int64\n",
      "SalePrice      1201 non-null int64\n",
      "Total Years    1201 non-null int64\n",
      "dtypes: float64(1), int32(3), int64(6)\n",
      "memory usage: 89.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For word embedding label encoding is really important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [5, 3, 1, 0],\n",
       "       ...,\n",
       "       [6, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [0, 3, 1, 3]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking and Converting into tensors\n",
    "import numpy as np\n",
    "\n",
    "cat_features = np.stack([df['MSSubClass'], df['MSZoning'], df['Street'], df['LotShape']],1)\n",
    "cat_features                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert numpy to tensor\n",
    "import torch\n",
    "\n",
    "cat_features = torch.tensor(cat_features, dtype=torch.int64)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create continuous variable\n",
    "\n",
    "cont_features = []\n",
    "for i in df.columns:\n",
    "    if i in ['MSSubClass', 'MSZoning', 'Street', 'LotShape', 'SalePrice']:\n",
    "        pass\n",
    "    else:\n",
    "        cont_features.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage', 'LotArea', '1stFlrSF', '2ndFlrSF', 'Total Years']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   65.,  8450.,   856.,   854.,    21.],\n",
       "        [   80.,  9600.,  1262.,     0.,    48.],\n",
       "        [   68., 11250.,   920.,   866.,    23.],\n",
       "        ...,\n",
       "        [   66.,  9042.,  1188.,  1152.,    83.],\n",
       "        [   68.,  9717.,  1078.,     0.,    74.],\n",
       "        [   75.,  9937.,  1256.,     0.,    59.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking continuous variable to a tensor\n",
    "\n",
    "cont_values = np.stack([df[i].values for i in cont_features], axis=1)\n",
    "cont_values = torch.tensor(cont_values, dtype=torch.float)\n",
    "cont_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[208500.],\n",
       "        [181500.],\n",
       "        [223500.],\n",
       "        ...,\n",
       "        [266500.],\n",
       "        [142125.],\n",
       "        [147500.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dependent Feature\n",
    "\n",
    "y = torch.tensor(df['SalePrice'].values, dtype=torch.float).reshape(-1, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1201 entries, 0 to 1459\n",
      "Data columns (total 10 columns):\n",
      "MSSubClass     1201 non-null int64\n",
      "MSZoning       1201 non-null int32\n",
      "LotFrontage    1201 non-null float64\n",
      "LotArea        1201 non-null int64\n",
      "Street         1201 non-null int32\n",
      "LotShape       1201 non-null int32\n",
      "1stFlrSF       1201 non-null int64\n",
      "2ndFlrSF       1201 non-null int64\n",
      "SalePrice      1201 non-null int64\n",
      "Total Years    1201 non-null int64\n",
      "dtypes: float64(1), int32(3), int64(6)\n",
      "memory usage: 89.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1201, 4]), torch.Size([1201, 5]), torch.Size([1201, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features.shape, cont_values.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['MSSubClass'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Size for categorical columns\n",
    "\n",
    "cat_dims = [len(df[col].unique()) for col in ['MSSubClass', 'MSZoning','Street', 'LotShape']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 5, 2, 4]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding is done categorical columns only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thumb Rule**: The output dimension of the embedding should be set based on the dimension as -->>  min(50, feature_dimension/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 8), (5, 3), (2, 1), (4, 2)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = [(length, min(50, (length+1) // 2)) for length in cat_dims]\n",
    "embedding_dim # (input_dimension, output_dimension) for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(15, 8)\n",
       "  (1): Embedding(5, 3)\n",
       "  (2): Embedding(2, 1)\n",
       "  (3): Embedding(4, 2)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "embed_representation = nn.ModuleList([nn.Embedding(inp,out) for inp, out in embedding_dim])\n",
    "\n",
    "embed_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        [6, 3, 1, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_featurez = cat_features[:4]\n",
    "cat_featurez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',500)\n",
    "embedding_val = []\n",
    "\n",
    "for i,e in enumerate(embed_representation):\n",
    "    embedding_val.append(e(cat_featurez[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.9032, -1.4331,  0.1095, -0.5064,  0.6002,  3.2089,  0.2396, -1.2482],\n",
       "         [ 1.6449,  0.7536,  0.9709, -1.1103,  0.0045,  0.0992, -1.1921,  0.2679],\n",
       "         [-1.9032, -1.4331,  0.1095, -0.5064,  0.6002,  3.2089,  0.2396, -1.2482],\n",
       "         [ 1.4656,  1.2097, -1.0281, -0.9983, -1.7537, -1.3650, -0.2872, -0.4146]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.3740, -1.2480, -0.7021],\n",
       "         [-0.3740, -1.2480, -0.7021],\n",
       "         [-0.3740, -1.2480, -0.7021],\n",
       "         [-0.3740, -1.2480, -0.7021]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.0862],\n",
       "         [-0.0862],\n",
       "         [-0.0862],\n",
       "         [-0.0862]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[ 1.9552,  0.1190],\n",
       "         [ 1.9552,  0.1190],\n",
       "         [-0.3502, -2.2070],\n",
       "         [-0.3502, -2.2070]], grad_fn=<EmbeddingBackward0>)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9032, -1.4331,  0.1095, -0.5064,  0.6002,  3.2089,  0.2396, -1.2482,\n",
       "         -0.3740, -1.2480, -0.7021, -0.0862,  1.9552,  0.1190],\n",
       "        [ 1.6449,  0.7536,  0.9709, -1.1103,  0.0045,  0.0992, -1.1921,  0.2679,\n",
       "         -0.3740, -1.2480, -0.7021, -0.0862,  1.9552,  0.1190],\n",
       "        [-1.9032, -1.4331,  0.1095, -0.5064,  0.6002,  3.2089,  0.2396, -1.2482,\n",
       "         -0.3740, -1.2480, -0.7021, -0.0862, -0.3502, -2.2070],\n",
       "        [ 1.4656,  1.2097, -1.0281, -0.9983, -1.7537, -1.3650, -0.2872, -0.4146,\n",
       "         -0.3740, -1.2480, -0.7021, -0.0862, -0.3502, -2.2070]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.cat(embedding_val, 1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement dropout\n",
    "dropout = nn.Dropout(.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.1720, -2.3885,  0.1825, -0.0000,  1.0003,  5.3482,  0.0000, -2.0804,\n",
       "         -0.6233, -0.0000, -0.0000, -0.0000,  3.2586,  0.1983],\n",
       "        [ 2.7415,  0.0000,  0.0000, -0.0000,  0.0000,  0.1653, -0.0000,  0.0000,\n",
       "         -0.0000, -2.0800, -1.1702, -0.0000,  3.2586,  0.1983],\n",
       "        [-0.0000, -0.0000,  0.0000, -0.8440,  0.0000,  5.3482,  0.3993, -2.0804,\n",
       "         -0.6233, -2.0800, -1.1702, -0.0000, -0.5836, -3.6783],\n",
       "        [ 2.4426,  2.0161, -0.0000, -1.6639, -2.9228, -2.2749, -0.4787, -0.6910,\n",
       "         -0.6233, -0.0000, -1.1702, -0.1436, -0.0000, -3.6783]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embed = dropout(z)\n",
    "final_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This embedding steps are to be followed while creating the neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Feed Forward Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, n_cont, out_sz, layers, p=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Creating the embeddings \n",
    "        self.embeds = nn.ModuleList([nn.Embedding(inp, out) for inp,out in embedding_dim])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "\n",
    "        # Batch Normalization\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "\n",
    "        layerlist = []\n",
    "        n_emb = sum((out for inp, out in embedding_dim))\n",
    "        n_in = n_emb + n_cont\n",
    "\n",
    "        # Creating the layers in the neural network\n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in, i))\n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "\n",
    "        layerlist.append(nn.Linear(layers[-1], out_sz))\n",
    "\n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        \n",
    "        x = torch.cat(embeddings,1)\n",
    "        x = self.emb_drop(x)\n",
    "\n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "model = FeedForwardNN(embedding_dim, len(cont_features), 1, [100,50], p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss() ### Later convert to RMSE\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   65.,  8450.,   856.,   854.,    21.],\n",
       "        [   80.,  9600.,  1262.,     0.,    48.],\n",
       "        [   68., 11250.,   920.,   866.,    23.],\n",
       "        ...,\n",
       "        [   66.,  9042.,  1188.,  1152.,    83.],\n",
       "        [   68.,  9717.,  1078.,     0.,    74.],\n",
       "        [   75.,  9937.,  1256.,     0.,    59.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1201, 5])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train Test Split the dataset\n",
    "batch_size = 1200\n",
    "test_size =int(batch_size *0.15)\n",
    "\n",
    "train_categorical = cat_features[:batch_size-test_size]\n",
    "test_categorical = cat_features[batch_size-test_size:batch_size]\n",
    "\n",
    "train_cont = cont_values[:batch_size-test_size]\n",
    "test_cont = cont_values[batch_size-test_size:batch_size]\n",
    "\n",
    "y_train = y[:batch_size-test_size]\n",
    "y_test = y[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 180, 1020, 180, 1020, 180)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_categorical), len(test_categorical), len(train_cont), len(test_cont), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 and the loss: 200496.75\n",
      "Epoch number: 11 and the loss: 200493.46875\n",
      "Epoch number: 21 and the loss: 200489.140625\n",
      "Epoch number: 31 and the loss: 200482.640625\n",
      "Epoch number: 41 and the loss: 200473.25\n",
      "Epoch number: 51 and the loss: 200461.375\n",
      "Epoch number: 61 and the loss: 200446.40625\n",
      "Epoch number: 71 and the loss: 200429.359375\n",
      "Epoch number: 81 and the loss: 200408.0\n",
      "Epoch number: 91 and the loss: 200383.421875\n",
      "Epoch number: 101 and the loss: 200355.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 111 and the loss: 200322.125\n",
      "Epoch number: 121 and the loss: 200291.4375\n",
      "Epoch number: 131 and the loss: 200252.015625\n",
      "Epoch number: 141 and the loss: 200206.625\n",
      "Epoch number: 151 and the loss: 200162.234375\n",
      "Epoch number: 161 and the loss: 200112.25\n",
      "Epoch number: 171 and the loss: 200059.6875\n",
      "Epoch number: 181 and the loss: 200005.875\n",
      "Epoch number: 191 and the loss: 199946.359375\n",
      "Epoch number: 201 and the loss: 199881.71875\n",
      "Epoch number: 211 and the loss: 199816.09375\n",
      "Epoch number: 221 and the loss: 199737.15625\n",
      "Epoch number: 231 and the loss: 199669.234375\n",
      "Epoch number: 241 and the loss: 199588.859375\n",
      "Epoch number: 251 and the loss: 199505.0\n",
      "Epoch number: 261 and the loss: 199410.078125\n",
      "Epoch number: 271 and the loss: 199323.0\n",
      "Epoch number: 281 and the loss: 199243.6875\n",
      "Epoch number: 291 and the loss: 199140.25\n",
      "Epoch number: 301 and the loss: 199027.765625\n",
      "Epoch number: 311 and the loss: 198931.578125\n",
      "Epoch number: 321 and the loss: 198852.484375\n",
      "Epoch number: 331 and the loss: 198703.359375\n",
      "Epoch number: 341 and the loss: 198607.515625\n",
      "Epoch number: 351 and the loss: 198491.0\n",
      "Epoch number: 361 and the loss: 198387.984375\n",
      "Epoch number: 371 and the loss: 198245.03125\n",
      "Epoch number: 381 and the loss: 198109.203125\n",
      "Epoch number: 391 and the loss: 198017.90625\n",
      "Epoch number: 401 and the loss: 197883.71875\n",
      "Epoch number: 411 and the loss: 197731.046875\n",
      "Epoch number: 421 and the loss: 197594.28125\n",
      "Epoch number: 431 and the loss: 197425.25\n",
      "Epoch number: 441 and the loss: 197288.265625\n",
      "Epoch number: 451 and the loss: 197172.140625\n",
      "Epoch number: 461 and the loss: 196966.21875\n",
      "Epoch number: 471 and the loss: 196880.953125\n",
      "Epoch number: 481 and the loss: 196706.625\n",
      "Epoch number: 491 and the loss: 196503.21875\n",
      "Epoch number: 501 and the loss: 196434.296875\n",
      "Epoch number: 511 and the loss: 196209.65625\n",
      "Epoch number: 521 and the loss: 196044.09375\n",
      "Epoch number: 531 and the loss: 195849.953125\n",
      "Epoch number: 541 and the loss: 195660.453125\n",
      "Epoch number: 551 and the loss: 195462.34375\n",
      "Epoch number: 561 and the loss: 195288.375\n",
      "Epoch number: 571 and the loss: 195071.21875\n",
      "Epoch number: 581 and the loss: 194840.8125\n",
      "Epoch number: 591 and the loss: 194689.375\n",
      "Epoch number: 601 and the loss: 194525.984375\n",
      "Epoch number: 611 and the loss: 194277.78125\n",
      "Epoch number: 621 and the loss: 194119.03125\n",
      "Epoch number: 631 and the loss: 193836.875\n",
      "Epoch number: 641 and the loss: 193683.921875\n",
      "Epoch number: 651 and the loss: 193482.484375\n",
      "Epoch number: 661 and the loss: 193219.546875\n",
      "Epoch number: 671 and the loss: 193061.5\n",
      "Epoch number: 681 and the loss: 192777.28125\n",
      "Epoch number: 691 and the loss: 192488.046875\n",
      "Epoch number: 701 and the loss: 192298.578125\n",
      "Epoch number: 711 and the loss: 192206.140625\n",
      "Epoch number: 721 and the loss: 191970.140625\n",
      "Epoch number: 731 and the loss: 191627.03125\n",
      "Epoch number: 741 and the loss: 191431.171875\n",
      "Epoch number: 751 and the loss: 191191.59375\n",
      "Epoch number: 761 and the loss: 190989.046875\n",
      "Epoch number: 771 and the loss: 190728.40625\n",
      "Epoch number: 781 and the loss: 190474.5625\n",
      "Epoch number: 791 and the loss: 190286.46875\n",
      "Epoch number: 801 and the loss: 189878.234375\n",
      "Epoch number: 811 and the loss: 189756.640625\n",
      "Epoch number: 821 and the loss: 189698.75\n",
      "Epoch number: 831 and the loss: 189293.296875\n",
      "Epoch number: 841 and the loss: 188947.796875\n",
      "Epoch number: 851 and the loss: 188845.1875\n",
      "Epoch number: 861 and the loss: 188395.796875\n",
      "Epoch number: 871 and the loss: 188172.015625\n",
      "Epoch number: 881 and the loss: 187890.484375\n",
      "Epoch number: 891 and the loss: 187482.3125\n",
      "Epoch number: 901 and the loss: 187441.84375\n",
      "Epoch number: 911 and the loss: 187240.359375\n",
      "Epoch number: 921 and the loss: 186793.609375\n",
      "Epoch number: 931 and the loss: 186368.140625\n",
      "Epoch number: 941 and the loss: 186158.359375\n",
      "Epoch number: 951 and the loss: 185965.203125\n",
      "Epoch number: 961 and the loss: 185541.53125\n",
      "Epoch number: 971 and the loss: 185408.96875\n",
      "Epoch number: 981 and the loss: 185073.078125\n",
      "Epoch number: 991 and the loss: 184783.890625\n",
      "Epoch number: 1001 and the loss: 184306.625\n",
      "Epoch number: 1011 and the loss: 184013.734375\n",
      "Epoch number: 1021 and the loss: 183794.28125\n",
      "Epoch number: 1031 and the loss: 183367.265625\n",
      "Epoch number: 1041 and the loss: 183220.921875\n",
      "Epoch number: 1051 and the loss: 182916.734375\n",
      "Epoch number: 1061 and the loss: 182564.53125\n",
      "Epoch number: 1071 and the loss: 182313.109375\n",
      "Epoch number: 1081 and the loss: 181890.375\n",
      "Epoch number: 1091 and the loss: 181496.390625\n",
      "Epoch number: 1101 and the loss: 181312.34375\n",
      "Epoch number: 1111 and the loss: 180741.234375\n",
      "Epoch number: 1121 and the loss: 180488.625\n",
      "Epoch number: 1131 and the loss: 180412.0\n",
      "Epoch number: 1141 and the loss: 179938.34375\n",
      "Epoch number: 1151 and the loss: 179544.515625\n",
      "Epoch number: 1161 and the loss: 179349.984375\n",
      "Epoch number: 1171 and the loss: 178931.578125\n",
      "Epoch number: 1181 and the loss: 178534.1875\n",
      "Epoch number: 1191 and the loss: 178107.140625\n",
      "Epoch number: 1201 and the loss: 178089.140625\n",
      "Epoch number: 1211 and the loss: 177446.1875\n",
      "Epoch number: 1221 and the loss: 177480.953125\n",
      "Epoch number: 1231 and the loss: 176572.515625\n",
      "Epoch number: 1241 and the loss: 176332.59375\n",
      "Epoch number: 1251 and the loss: 175544.0625\n",
      "Epoch number: 1261 and the loss: 175573.296875\n",
      "Epoch number: 1271 and the loss: 175237.515625\n",
      "Epoch number: 1281 and the loss: 174925.328125\n",
      "Epoch number: 1291 and the loss: 174458.859375\n",
      "Epoch number: 1301 and the loss: 174197.703125\n",
      "Epoch number: 1311 and the loss: 173675.546875\n",
      "Epoch number: 1321 and the loss: 173118.3125\n",
      "Epoch number: 1331 and the loss: 173385.234375\n",
      "Epoch number: 1341 and the loss: 172634.5\n",
      "Epoch number: 1351 and the loss: 172267.9375\n",
      "Epoch number: 1361 and the loss: 172005.234375\n",
      "Epoch number: 1371 and the loss: 171836.75\n",
      "Epoch number: 1381 and the loss: 171295.59375\n",
      "Epoch number: 1391 and the loss: 170342.609375\n",
      "Epoch number: 1401 and the loss: 170513.703125\n",
      "Epoch number: 1411 and the loss: 169674.828125\n",
      "Epoch number: 1421 and the loss: 169865.125\n",
      "Epoch number: 1431 and the loss: 169466.9375\n",
      "Epoch number: 1441 and the loss: 168686.359375\n",
      "Epoch number: 1451 and the loss: 168608.5\n",
      "Epoch number: 1461 and the loss: 167965.59375\n",
      "Epoch number: 1471 and the loss: 167596.09375\n",
      "Epoch number: 1481 and the loss: 167358.71875\n",
      "Epoch number: 1491 and the loss: 166550.046875\n",
      "Epoch number: 1501 and the loss: 166348.21875\n",
      "Epoch number: 1511 and the loss: 165564.765625\n",
      "Epoch number: 1521 and the loss: 165859.875\n",
      "Epoch number: 1531 and the loss: 165285.875\n",
      "Epoch number: 1541 and the loss: 164584.625\n",
      "Epoch number: 1551 and the loss: 164549.203125\n",
      "Epoch number: 1561 and the loss: 163728.34375\n",
      "Epoch number: 1571 and the loss: 163478.5625\n",
      "Epoch number: 1581 and the loss: 163085.390625\n",
      "Epoch number: 1591 and the loss: 162965.203125\n",
      "Epoch number: 1601 and the loss: 162445.515625\n",
      "Epoch number: 1611 and the loss: 161701.40625\n",
      "Epoch number: 1621 and the loss: 161095.71875\n",
      "Epoch number: 1631 and the loss: 160809.421875\n",
      "Epoch number: 1641 and the loss: 160627.28125\n",
      "Epoch number: 1651 and the loss: 160240.375\n",
      "Epoch number: 1661 and the loss: 160221.625\n",
      "Epoch number: 1671 and the loss: 159050.171875\n",
      "Epoch number: 1681 and the loss: 158426.59375\n",
      "Epoch number: 1691 and the loss: 158313.25\n",
      "Epoch number: 1701 and the loss: 157646.515625\n",
      "Epoch number: 1711 and the loss: 157362.90625\n",
      "Epoch number: 1721 and the loss: 157175.390625\n",
      "Epoch number: 1731 and the loss: 157076.515625\n",
      "Epoch number: 1741 and the loss: 156364.28125\n",
      "Epoch number: 1751 and the loss: 156124.578125\n",
      "Epoch number: 1761 and the loss: 154801.453125\n",
      "Epoch number: 1771 and the loss: 154318.203125\n",
      "Epoch number: 1781 and the loss: 154102.765625\n",
      "Epoch number: 1791 and the loss: 153764.84375\n",
      "Epoch number: 1801 and the loss: 153269.234375\n",
      "Epoch number: 1811 and the loss: 152841.25\n",
      "Epoch number: 1821 and the loss: 152364.296875\n",
      "Epoch number: 1831 and the loss: 152469.265625\n",
      "Epoch number: 1841 and the loss: 151632.75\n",
      "Epoch number: 1851 and the loss: 150662.546875\n",
      "Epoch number: 1861 and the loss: 150833.703125\n",
      "Epoch number: 1871 and the loss: 150042.421875\n",
      "Epoch number: 1881 and the loss: 149300.28125\n",
      "Epoch number: 1891 and the loss: 148371.625\n",
      "Epoch number: 1901 and the loss: 148650.71875\n",
      "Epoch number: 1911 and the loss: 148931.359375\n",
      "Epoch number: 1921 and the loss: 147737.140625\n",
      "Epoch number: 1931 and the loss: 147340.640625\n",
      "Epoch number: 1941 and the loss: 146649.75\n",
      "Epoch number: 1951 and the loss: 146793.84375\n",
      "Epoch number: 1961 and the loss: 145955.234375\n",
      "Epoch number: 1971 and the loss: 145609.09375\n",
      "Epoch number: 1981 and the loss: 145715.5\n",
      "Epoch number: 1991 and the loss: 144495.671875\n",
      "Epoch number: 2001 and the loss: 144149.265625\n",
      "Epoch number: 2011 and the loss: 143064.515625\n",
      "Epoch number: 2021 and the loss: 143039.484375\n",
      "Epoch number: 2031 and the loss: 142518.46875\n",
      "Epoch number: 2041 and the loss: 141961.875\n",
      "Epoch number: 2051 and the loss: 141059.328125\n",
      "Epoch number: 2061 and the loss: 140967.984375\n",
      "Epoch number: 2071 and the loss: 141000.015625\n",
      "Epoch number: 2081 and the loss: 139717.875\n",
      "Epoch number: 2091 and the loss: 139809.078125\n",
      "Epoch number: 2101 and the loss: 139439.859375\n",
      "Epoch number: 2111 and the loss: 138721.234375\n",
      "Epoch number: 2121 and the loss: 137854.109375\n",
      "Epoch number: 2131 and the loss: 137945.625\n",
      "Epoch number: 2141 and the loss: 137219.3125\n",
      "Epoch number: 2151 and the loss: 136643.1875\n",
      "Epoch number: 2161 and the loss: 136439.65625\n",
      "Epoch number: 2171 and the loss: 136618.171875\n",
      "Epoch number: 2181 and the loss: 135977.640625\n",
      "Epoch number: 2191 and the loss: 134920.53125\n",
      "Epoch number: 2201 and the loss: 134096.9375\n",
      "Epoch number: 2211 and the loss: 133813.4375\n",
      "Epoch number: 2221 and the loss: 132810.0\n",
      "Epoch number: 2231 and the loss: 132685.0\n",
      "Epoch number: 2241 and the loss: 132270.234375\n",
      "Epoch number: 2251 and the loss: 131676.65625\n",
      "Epoch number: 2261 and the loss: 130303.4296875\n",
      "Epoch number: 2271 and the loss: 130279.0234375\n",
      "Epoch number: 2281 and the loss: 130298.265625\n",
      "Epoch number: 2291 and the loss: 129494.609375\n",
      "Epoch number: 2301 and the loss: 129009.765625\n",
      "Epoch number: 2311 and the loss: 128937.9453125\n",
      "Epoch number: 2321 and the loss: 129333.765625\n",
      "Epoch number: 2331 and the loss: 128627.65625\n",
      "Epoch number: 2341 and the loss: 127626.4453125\n",
      "Epoch number: 2351 and the loss: 126241.1875\n",
      "Epoch number: 2361 and the loss: 127194.625\n",
      "Epoch number: 2371 and the loss: 125703.953125\n",
      "Epoch number: 2381 and the loss: 124769.0\n",
      "Epoch number: 2391 and the loss: 126580.2578125\n",
      "Epoch number: 2401 and the loss: 124398.78125\n",
      "Epoch number: 2411 and the loss: 123218.7578125\n",
      "Epoch number: 2421 and the loss: 123292.5625\n",
      "Epoch number: 2431 and the loss: 122414.3125\n",
      "Epoch number: 2441 and the loss: 122412.875\n",
      "Epoch number: 2451 and the loss: 121396.390625\n",
      "Epoch number: 2461 and the loss: 120458.7109375\n",
      "Epoch number: 2471 and the loss: 120846.09375\n",
      "Epoch number: 2481 and the loss: 120447.140625\n",
      "Epoch number: 2491 and the loss: 121773.75\n",
      "Epoch number: 2501 and the loss: 119595.296875\n",
      "Epoch number: 2511 and the loss: 118673.515625\n",
      "Epoch number: 2521 and the loss: 117641.8984375\n",
      "Epoch number: 2531 and the loss: 118325.5625\n",
      "Epoch number: 2541 and the loss: 117440.4921875\n",
      "Epoch number: 2551 and the loss: 118641.4765625\n",
      "Epoch number: 2561 and the loss: 116291.296875\n",
      "Epoch number: 2571 and the loss: 115839.6953125\n",
      "Epoch number: 2581 and the loss: 115878.015625\n",
      "Epoch number: 2591 and the loss: 115477.25\n",
      "Epoch number: 2601 and the loss: 114450.296875\n",
      "Epoch number: 2611 and the loss: 114358.8828125\n",
      "Epoch number: 2621 and the loss: 114559.9765625\n",
      "Epoch number: 2631 and the loss: 112505.1484375\n",
      "Epoch number: 2641 and the loss: 112214.953125\n",
      "Epoch number: 2651 and the loss: 112164.6796875\n",
      "Epoch number: 2661 and the loss: 111547.84375\n",
      "Epoch number: 2671 and the loss: 110787.5\n",
      "Epoch number: 2681 and the loss: 110071.859375\n",
      "Epoch number: 2691 and the loss: 109400.75\n",
      "Epoch number: 2701 and the loss: 109723.9921875\n",
      "Epoch number: 2711 and the loss: 109305.7265625\n",
      "Epoch number: 2721 and the loss: 108889.1328125\n",
      "Epoch number: 2731 and the loss: 108575.4453125\n",
      "Epoch number: 2741 and the loss: 107232.3046875\n",
      "Epoch number: 2751 and the loss: 105768.328125\n",
      "Epoch number: 2761 and the loss: 106355.1015625\n",
      "Epoch number: 2771 and the loss: 106693.71875\n",
      "Epoch number: 2781 and the loss: 107076.875\n",
      "Epoch number: 2791 and the loss: 104646.953125\n",
      "Epoch number: 2801 and the loss: 103620.9375\n",
      "Epoch number: 2811 and the loss: 103976.3203125\n",
      "Epoch number: 2821 and the loss: 102725.8125\n",
      "Epoch number: 2831 and the loss: 102806.2890625\n",
      "Epoch number: 2841 and the loss: 102340.2109375\n",
      "Epoch number: 2851 and the loss: 101223.3515625\n",
      "Epoch number: 2861 and the loss: 101962.46875\n",
      "Epoch number: 2871 and the loss: 100630.21875\n",
      "Epoch number: 2881 and the loss: 100979.84375\n",
      "Epoch number: 2891 and the loss: 100375.4296875\n",
      "Epoch number: 2901 and the loss: 98695.15625\n",
      "Epoch number: 2911 and the loss: 97871.3671875\n",
      "Epoch number: 2921 and the loss: 98253.609375\n",
      "Epoch number: 2931 and the loss: 98805.625\n",
      "Epoch number: 2941 and the loss: 97047.71875\n",
      "Epoch number: 2951 and the loss: 97437.7265625\n",
      "Epoch number: 2961 and the loss: 96362.953125\n",
      "Epoch number: 2971 and the loss: 95385.1953125\n",
      "Epoch number: 2981 and the loss: 95233.0078125\n",
      "Epoch number: 2991 and the loss: 94187.734375\n",
      "Epoch number: 3001 and the loss: 93821.078125\n",
      "Epoch number: 3011 and the loss: 94167.1171875\n",
      "Epoch number: 3021 and the loss: 96116.0078125\n",
      "Epoch number: 3031 and the loss: 92801.9375\n",
      "Epoch number: 3041 and the loss: 92966.921875\n",
      "Epoch number: 3051 and the loss: 91373.9296875\n",
      "Epoch number: 3061 and the loss: 91961.40625\n",
      "Epoch number: 3071 and the loss: 91161.7734375\n",
      "Epoch number: 3081 and the loss: 90838.1953125\n",
      "Epoch number: 3091 and the loss: 89760.8125\n",
      "Epoch number: 3101 and the loss: 89422.3125\n",
      "Epoch number: 3111 and the loss: 89139.390625\n",
      "Epoch number: 3121 and the loss: 89307.2265625\n",
      "Epoch number: 3131 and the loss: 87913.578125\n",
      "Epoch number: 3141 and the loss: 87751.8203125\n",
      "Epoch number: 3151 and the loss: 87535.9765625\n",
      "Epoch number: 3161 and the loss: 86119.2421875\n",
      "Epoch number: 3171 and the loss: 87028.8984375\n",
      "Epoch number: 3181 and the loss: 86091.046875\n",
      "Epoch number: 3191 and the loss: 85251.6875\n",
      "Epoch number: 3201 and the loss: 85245.9609375\n",
      "Epoch number: 3211 and the loss: 83998.1015625\n",
      "Epoch number: 3221 and the loss: 82958.2265625\n",
      "Epoch number: 3231 and the loss: 84326.46875\n",
      "Epoch number: 3241 and the loss: 82213.4296875\n",
      "Epoch number: 3251 and the loss: 81723.0859375\n",
      "Epoch number: 3261 and the loss: 81449.2890625\n",
      "Epoch number: 3271 and the loss: 81805.8046875\n",
      "Epoch number: 3281 and the loss: 80989.265625\n",
      "Epoch number: 3291 and the loss: 80632.46875\n",
      "Epoch number: 3301 and the loss: 79918.265625\n",
      "Epoch number: 3311 and the loss: 78998.2734375\n",
      "Epoch number: 3321 and the loss: 79320.34375\n",
      "Epoch number: 3331 and the loss: 78355.5546875\n",
      "Epoch number: 3341 and the loss: 77465.203125\n",
      "Epoch number: 3351 and the loss: 76981.5625\n",
      "Epoch number: 3361 and the loss: 76848.6875\n",
      "Epoch number: 3371 and the loss: 76307.515625\n",
      "Epoch number: 3381 and the loss: 76070.0546875\n",
      "Epoch number: 3391 and the loss: 74763.5\n",
      "Epoch number: 3401 and the loss: 74158.578125\n",
      "Epoch number: 3411 and the loss: 74560.140625\n",
      "Epoch number: 3421 and the loss: 73493.3984375\n",
      "Epoch number: 3431 and the loss: 73658.953125\n",
      "Epoch number: 3441 and the loss: 72542.8984375\n",
      "Epoch number: 3451 and the loss: 72863.8359375\n",
      "Epoch number: 3461 and the loss: 72595.9453125\n",
      "Epoch number: 3471 and the loss: 70642.0\n",
      "Epoch number: 3481 and the loss: 70748.609375\n",
      "Epoch number: 3491 and the loss: 71091.546875\n",
      "Epoch number: 3501 and the loss: 68621.0703125\n",
      "Epoch number: 3511 and the loss: 69046.03125\n",
      "Epoch number: 3521 and the loss: 68479.1484375\n",
      "Epoch number: 3531 and the loss: 68397.1015625\n",
      "Epoch number: 3541 and the loss: 68123.65625\n",
      "Epoch number: 3551 and the loss: 67591.6015625\n",
      "Epoch number: 3561 and the loss: 66033.1875\n",
      "Epoch number: 3571 and the loss: 66445.3984375\n",
      "Epoch number: 3581 and the loss: 65834.6171875\n",
      "Epoch number: 3591 and the loss: 66243.9140625\n",
      "Epoch number: 3601 and the loss: 64570.38671875\n",
      "Epoch number: 3611 and the loss: 64928.51953125\n",
      "Epoch number: 3621 and the loss: 65222.74609375\n",
      "Epoch number: 3631 and the loss: 62681.6796875\n",
      "Epoch number: 3641 and the loss: 64398.80859375\n",
      "Epoch number: 3651 and the loss: 62349.375\n",
      "Epoch number: 3661 and the loss: 63741.14453125\n",
      "Epoch number: 3671 and the loss: 61390.859375\n",
      "Epoch number: 3681 and the loss: 61027.42578125\n",
      "Epoch number: 3691 and the loss: 61586.7578125\n",
      "Epoch number: 3701 and the loss: 60320.66015625\n",
      "Epoch number: 3711 and the loss: 61934.71875\n",
      "Epoch number: 3721 and the loss: 59659.2734375\n",
      "Epoch number: 3731 and the loss: 57482.52734375\n",
      "Epoch number: 3741 and the loss: 57880.48828125\n",
      "Epoch number: 3751 and the loss: 59145.15234375\n",
      "Epoch number: 3761 and the loss: 58800.0390625\n",
      "Epoch number: 3771 and the loss: 57005.2265625\n",
      "Epoch number: 3781 and the loss: 55665.65234375\n",
      "Epoch number: 3791 and the loss: 56967.87890625\n",
      "Epoch number: 3801 and the loss: 56246.98046875\n",
      "Epoch number: 3811 and the loss: 55306.82421875\n",
      "Epoch number: 3821 and the loss: 55943.08984375\n",
      "Epoch number: 3831 and the loss: 54415.06640625\n",
      "Epoch number: 3841 and the loss: 53064.015625\n",
      "Epoch number: 3851 and the loss: 55570.98828125\n",
      "Epoch number: 3861 and the loss: 54130.96875\n",
      "Epoch number: 3871 and the loss: 52235.0859375\n",
      "Epoch number: 3881 and the loss: 51137.6953125\n",
      "Epoch number: 3891 and the loss: 50751.66015625\n",
      "Epoch number: 3901 and the loss: 55977.43359375\n",
      "Epoch number: 3911 and the loss: 52514.2890625\n",
      "Epoch number: 3921 and the loss: 50970.64453125\n",
      "Epoch number: 3931 and the loss: 50455.6484375\n",
      "Epoch number: 3941 and the loss: 49661.3671875\n",
      "Epoch number: 3951 and the loss: 49208.65625\n",
      "Epoch number: 3961 and the loss: 50491.078125\n",
      "Epoch number: 3971 and the loss: 49080.69921875\n",
      "Epoch number: 3981 and the loss: 49317.53125\n",
      "Epoch number: 3991 and the loss: 49786.2265625\n",
      "Epoch number: 4001 and the loss: 48186.3359375\n",
      "Epoch number: 4011 and the loss: 46511.05859375\n",
      "Epoch number: 4021 and the loss: 48660.33203125\n",
      "Epoch number: 4031 and the loss: 46375.57421875\n",
      "Epoch number: 4041 and the loss: 46523.27734375\n",
      "Epoch number: 4051 and the loss: 47429.1953125\n",
      "Epoch number: 4061 and the loss: 45019.30859375\n",
      "Epoch number: 4071 and the loss: 45494.5234375\n",
      "Epoch number: 4081 and the loss: 44584.109375\n",
      "Epoch number: 4091 and the loss: 45127.75390625\n",
      "Epoch number: 4101 and the loss: 43351.24609375\n",
      "Epoch number: 4111 and the loss: 44153.26171875\n",
      "Epoch number: 4121 and the loss: 44488.47265625\n",
      "Epoch number: 4131 and the loss: 43522.85546875\n",
      "Epoch number: 4141 and the loss: 44400.84765625\n",
      "Epoch number: 4151 and the loss: 43224.70703125\n",
      "Epoch number: 4161 and the loss: 42070.53125\n",
      "Epoch number: 4171 and the loss: 41767.2265625\n",
      "Epoch number: 4181 and the loss: 42291.19140625\n",
      "Epoch number: 4191 and the loss: 42309.85546875\n",
      "Epoch number: 4201 and the loss: 41331.7890625\n",
      "Epoch number: 4211 and the loss: 40400.1953125\n",
      "Epoch number: 4221 and the loss: 39495.33203125\n",
      "Epoch number: 4231 and the loss: 41696.1015625\n",
      "Epoch number: 4241 and the loss: 40366.3984375\n",
      "Epoch number: 4251 and the loss: 39737.44921875\n",
      "Epoch number: 4261 and the loss: 40778.78515625\n",
      "Epoch number: 4271 and the loss: 40608.41796875\n",
      "Epoch number: 4281 and the loss: 41646.20703125\n",
      "Epoch number: 4291 and the loss: 38906.69140625\n",
      "Epoch number: 4301 and the loss: 39401.984375\n",
      "Epoch number: 4311 and the loss: 39361.01171875\n",
      "Epoch number: 4321 and the loss: 40109.73828125\n",
      "Epoch number: 4331 and the loss: 38880.75390625\n",
      "Epoch number: 4341 and the loss: 38381.6328125\n",
      "Epoch number: 4351 and the loss: 41234.8671875\n",
      "Epoch number: 4361 and the loss: 38868.078125\n",
      "Epoch number: 4371 and the loss: 38881.7890625\n",
      "Epoch number: 4381 and the loss: 38738.26953125\n",
      "Epoch number: 4391 and the loss: 36080.1796875\n",
      "Epoch number: 4401 and the loss: 36827.81640625\n",
      "Epoch number: 4411 and the loss: 38580.125\n",
      "Epoch number: 4421 and the loss: 38814.296875\n",
      "Epoch number: 4431 and the loss: 37912.6328125\n",
      "Epoch number: 4441 and the loss: 37690.8984375\n",
      "Epoch number: 4451 and the loss: 37729.41796875\n",
      "Epoch number: 4461 and the loss: 37278.06640625\n",
      "Epoch number: 4471 and the loss: 36962.6875\n",
      "Epoch number: 4481 and the loss: 36619.16796875\n",
      "Epoch number: 4491 and the loss: 36689.2265625\n",
      "Epoch number: 4501 and the loss: 37661.79296875\n",
      "Epoch number: 4511 and the loss: 37363.09375\n",
      "Epoch number: 4521 and the loss: 36583.53515625\n",
      "Epoch number: 4531 and the loss: 36690.44921875\n",
      "Epoch number: 4541 and the loss: 35368.421875\n",
      "Epoch number: 4551 and the loss: 35966.109375\n",
      "Epoch number: 4561 and the loss: 35896.95703125\n",
      "Epoch number: 4571 and the loss: 38275.80859375\n",
      "Epoch number: 4581 and the loss: 35448.2421875\n",
      "Epoch number: 4591 and the loss: 34030.80859375\n",
      "Epoch number: 4601 and the loss: 36417.296875\n",
      "Epoch number: 4611 and the loss: 35842.44140625\n",
      "Epoch number: 4621 and the loss: 35346.59375\n",
      "Epoch number: 4631 and the loss: 35982.73828125\n",
      "Epoch number: 4641 and the loss: 35131.95703125\n",
      "Epoch number: 4651 and the loss: 37382.0703125\n",
      "Epoch number: 4661 and the loss: 33924.91796875\n",
      "Epoch number: 4671 and the loss: 36430.3515625\n",
      "Epoch number: 4681 and the loss: 35404.37890625\n",
      "Epoch number: 4691 and the loss: 38356.671875\n",
      "Epoch number: 4701 and the loss: 34638.17578125\n",
      "Epoch number: 4711 and the loss: 33851.6171875\n",
      "Epoch number: 4721 and the loss: 34962.20703125\n",
      "Epoch number: 4731 and the loss: 36103.0546875\n",
      "Epoch number: 4741 and the loss: 35054.21875\n",
      "Epoch number: 4751 and the loss: 35044.01953125\n",
      "Epoch number: 4761 and the loss: 35571.296875\n",
      "Epoch number: 4771 and the loss: 34987.890625\n",
      "Epoch number: 4781 and the loss: 33631.51953125\n",
      "Epoch number: 4791 and the loss: 35513.86328125\n",
      "Epoch number: 4801 and the loss: 34645.08203125\n",
      "Epoch number: 4811 and the loss: 34602.71875\n",
      "Epoch number: 4821 and the loss: 35271.0078125\n",
      "Epoch number: 4831 and the loss: 35653.9609375\n",
      "Epoch number: 4841 and the loss: 34617.40234375\n",
      "Epoch number: 4851 and the loss: 36409.74609375\n",
      "Epoch number: 4861 and the loss: 36238.39453125\n",
      "Epoch number: 4871 and the loss: 34886.5234375\n",
      "Epoch number: 4881 and the loss: 36695.125\n",
      "Epoch number: 4891 and the loss: 35493.4375\n",
      "Epoch number: 4901 and the loss: 35922.40234375\n",
      "Epoch number: 4911 and the loss: 33638.25\n",
      "Epoch number: 4921 and the loss: 34609.97265625\n",
      "Epoch number: 4931 and the loss: 35868.01171875\n",
      "Epoch number: 4941 and the loss: 36003.57421875\n",
      "Epoch number: 4951 and the loss: 32810.96484375\n",
      "Epoch number: 4961 and the loss: 34659.5546875\n",
      "Epoch number: 4971 and the loss: 35653.1015625\n",
      "Epoch number: 4981 and the loss: 34556.23046875\n",
      "Epoch number: 4991 and the loss: 32753.490234375\n",
      "Epoch number: 5001 and the loss: 33744.5703125\n",
      "Epoch number: 5011 and the loss: 33939.71875\n",
      "Epoch number: 5021 and the loss: 35360.0703125\n",
      "Epoch number: 5031 and the loss: 34751.86328125\n",
      "Epoch number: 5041 and the loss: 33058.4375\n",
      "Epoch number: 5051 and the loss: 34027.5234375\n",
      "Epoch number: 5061 and the loss: 35341.8046875\n",
      "Epoch number: 5071 and the loss: 33358.8046875\n",
      "Epoch number: 5081 and the loss: 34893.3828125\n",
      "Epoch number: 5091 and the loss: 36108.55078125\n",
      "Epoch number: 5101 and the loss: 36062.93359375\n",
      "Epoch number: 5111 and the loss: 34959.19140625\n",
      "Epoch number: 5121 and the loss: 35732.70703125\n",
      "Epoch number: 5131 and the loss: 35892.18359375\n",
      "Epoch number: 5141 and the loss: 35321.921875\n",
      "Epoch number: 5151 and the loss: 35333.8046875\n",
      "Epoch number: 5161 and the loss: 34158.671875\n",
      "Epoch number: 5171 and the loss: 35399.65625\n",
      "Epoch number: 5181 and the loss: 34928.05078125\n",
      "Epoch number: 5191 and the loss: 34307.734375\n",
      "Epoch number: 5201 and the loss: 34313.64453125\n",
      "Epoch number: 5211 and the loss: 33588.44140625\n",
      "Epoch number: 5221 and the loss: 36240.90625\n",
      "Epoch number: 5231 and the loss: 33719.89453125\n",
      "Epoch number: 5241 and the loss: 33403.734375\n",
      "Epoch number: 5251 and the loss: 34380.5\n",
      "Epoch number: 5261 and the loss: 34171.97265625\n",
      "Epoch number: 5271 and the loss: 34707.66015625\n",
      "Epoch number: 5281 and the loss: 33260.234375\n",
      "Epoch number: 5291 and the loss: 33744.24609375\n",
      "Epoch number: 5301 and the loss: 34438.82421875\n",
      "Epoch number: 5311 and the loss: 35049.35546875\n",
      "Epoch number: 5321 and the loss: 33873.6796875\n",
      "Epoch number: 5331 and the loss: 35657.09375\n",
      "Epoch number: 5341 and the loss: 33801.2734375\n",
      "Epoch number: 5351 and the loss: 34663.9375\n",
      "Epoch number: 5361 and the loss: 34598.765625\n",
      "Epoch number: 5371 and the loss: 34735.43359375\n",
      "Epoch number: 5381 and the loss: 35490.88671875\n",
      "Epoch number: 5391 and the loss: 35911.3515625\n",
      "Epoch number: 5401 and the loss: 34917.7421875\n",
      "Epoch number: 5411 and the loss: 35866.73828125\n",
      "Epoch number: 5421 and the loss: 34217.69140625\n",
      "Epoch number: 5431 and the loss: 33203.6328125\n",
      "Epoch number: 5441 and the loss: 32661.5546875\n",
      "Epoch number: 5451 and the loss: 32550.849609375\n",
      "Epoch number: 5461 and the loss: 32707.44140625\n",
      "Epoch number: 5471 and the loss: 35195.20703125\n",
      "Epoch number: 5481 and the loss: 34247.06640625\n",
      "Epoch number: 5491 and the loss: 33593.5234375\n",
      "Epoch number: 5501 and the loss: 34825.7578125\n",
      "Epoch number: 5511 and the loss: 35736.828125\n",
      "Epoch number: 5521 and the loss: 35092.82421875\n",
      "Epoch number: 5531 and the loss: 34671.5078125\n",
      "Epoch number: 5541 and the loss: 34364.10546875\n",
      "Epoch number: 5551 and the loss: 35027.76953125\n",
      "Epoch number: 5561 and the loss: 33547.46875\n",
      "Epoch number: 5571 and the loss: 34358.99609375\n",
      "Epoch number: 5581 and the loss: 34255.234375\n",
      "Epoch number: 5591 and the loss: 34847.02734375\n",
      "Epoch number: 5601 and the loss: 33444.37890625\n",
      "Epoch number: 5611 and the loss: 35006.4765625\n",
      "Epoch number: 5621 and the loss: 32642.984375\n",
      "Epoch number: 5631 and the loss: 34381.0\n",
      "Epoch number: 5641 and the loss: 35752.24609375\n",
      "Epoch number: 5651 and the loss: 34409.65234375\n",
      "Epoch number: 5661 and the loss: 34761.7109375\n",
      "Epoch number: 5671 and the loss: 33048.8671875\n",
      "Epoch number: 5681 and the loss: 33761.25\n",
      "Epoch number: 5691 and the loss: 33635.0390625\n",
      "Epoch number: 5701 and the loss: 42316.703125\n",
      "Epoch number: 5711 and the loss: 33959.859375\n",
      "Epoch number: 5721 and the loss: 32375.78515625\n",
      "Epoch number: 5731 and the loss: 34494.58203125\n",
      "Epoch number: 5741 and the loss: 33803.0703125\n",
      "Epoch number: 5751 and the loss: 35172.125\n",
      "Epoch number: 5761 and the loss: 33457.64453125\n",
      "Epoch number: 5771 and the loss: 35990.7265625\n",
      "Epoch number: 5781 and the loss: 33541.96875\n",
      "Epoch number: 5791 and the loss: 34306.91015625\n",
      "Epoch number: 5801 and the loss: 35152.16796875\n",
      "Epoch number: 5811 and the loss: 33271.12109375\n",
      "Epoch number: 5821 and the loss: 33190.53125\n",
      "Epoch number: 5831 and the loss: 34153.70703125\n",
      "Epoch number: 5841 and the loss: 36223.33203125\n",
      "Epoch number: 5851 and the loss: 36379.96484375\n",
      "Epoch number: 5861 and the loss: 34384.44140625\n",
      "Epoch number: 5871 and the loss: 34256.3046875\n",
      "Epoch number: 5881 and the loss: 35619.734375\n",
      "Epoch number: 5891 and the loss: 33762.60546875\n",
      "Epoch number: 5901 and the loss: 34471.203125\n",
      "Epoch number: 5911 and the loss: 34403.4375\n",
      "Epoch number: 5921 and the loss: 34620.01171875\n",
      "Epoch number: 5931 and the loss: 32821.01171875\n",
      "Epoch number: 5941 and the loss: 36740.2421875\n",
      "Epoch number: 5951 and the loss: 33503.44140625\n",
      "Epoch number: 5961 and the loss: 34610.71875\n",
      "Epoch number: 5971 and the loss: 33144.78125\n",
      "Epoch number: 5981 and the loss: 36008.9140625\n",
      "Epoch number: 5991 and the loss: 33626.46875\n",
      "Epoch number: 6001 and the loss: 34544.453125\n",
      "Epoch number: 6011 and the loss: 33875.15625\n",
      "Epoch number: 6021 and the loss: 35376.875\n",
      "Epoch number: 6031 and the loss: 34741.15234375\n",
      "Epoch number: 6041 and the loss: 33788.86328125\n",
      "Epoch number: 6051 and the loss: 34469.94140625\n",
      "Epoch number: 6061 and the loss: 33936.55078125\n",
      "Epoch number: 6071 and the loss: 34279.359375\n",
      "Epoch number: 6081 and the loss: 35245.23828125\n",
      "Epoch number: 6091 and the loss: 36971.640625\n",
      "Epoch number: 6101 and the loss: 34291.97265625\n",
      "Epoch number: 6111 and the loss: 34025.30078125\n",
      "Epoch number: 6121 and the loss: 35209.05859375\n",
      "Epoch number: 6131 and the loss: 34430.09765625\n",
      "Epoch number: 6141 and the loss: 34212.25390625\n",
      "Epoch number: 6151 and the loss: 34073.27734375\n",
      "Epoch number: 6161 and the loss: 33794.08984375\n",
      "Epoch number: 6171 and the loss: 33649.484375\n",
      "Epoch number: 6181 and the loss: 33882.7421875\n",
      "Epoch number: 6191 and the loss: 34911.62109375\n",
      "Epoch number: 6201 and the loss: 35754.14453125\n",
      "Epoch number: 6211 and the loss: 34801.03125\n",
      "Epoch number: 6221 and the loss: 35099.3984375\n",
      "Epoch number: 6231 and the loss: 34022.8828125\n",
      "Epoch number: 6241 and the loss: 34650.875\n",
      "Epoch number: 6251 and the loss: 34616.94140625\n",
      "Epoch number: 6261 and the loss: 34001.37890625\n",
      "Epoch number: 6271 and the loss: 32601.806640625\n",
      "Epoch number: 6281 and the loss: 34062.81640625\n",
      "Epoch number: 6291 and the loss: 33683.078125\n",
      "Epoch number: 6301 and the loss: 34006.2265625\n",
      "Epoch number: 6311 and the loss: 34337.1875\n",
      "Epoch number: 6321 and the loss: 34125.484375\n",
      "Epoch number: 6331 and the loss: 35073.15234375\n",
      "Epoch number: 6341 and the loss: 32635.029296875\n",
      "Epoch number: 6351 and the loss: 32866.984375\n",
      "Epoch number: 6361 and the loss: 33864.09765625\n",
      "Epoch number: 6371 and the loss: 31947.548828125\n",
      "Epoch number: 6381 and the loss: 33835.1484375\n",
      "Epoch number: 6391 and the loss: 34746.18359375\n",
      "Epoch number: 6401 and the loss: 33234.734375\n",
      "Epoch number: 6411 and the loss: 35795.78515625\n",
      "Epoch number: 6421 and the loss: 33417.40625\n",
      "Epoch number: 6431 and the loss: 33576.25390625\n",
      "Epoch number: 6441 and the loss: 34555.7421875\n",
      "Epoch number: 6451 and the loss: 31826.5\n",
      "Epoch number: 6461 and the loss: 34765.59765625\n",
      "Epoch number: 6471 and the loss: 34561.9375\n",
      "Epoch number: 6481 and the loss: 34172.0390625\n",
      "Epoch number: 6491 and the loss: 33834.3203125\n",
      "Epoch number: 6501 and the loss: 34223.1640625\n",
      "Epoch number: 6511 and the loss: 35347.96875\n",
      "Epoch number: 6521 and the loss: 36737.69140625\n",
      "Epoch number: 6531 and the loss: 34234.12890625\n",
      "Epoch number: 6541 and the loss: 33294.00390625\n",
      "Epoch number: 6551 and the loss: 34571.4921875\n",
      "Epoch number: 6561 and the loss: 33383.46484375\n",
      "Epoch number: 6571 and the loss: 35140.4375\n",
      "Epoch number: 6581 and the loss: 33634.8828125\n",
      "Epoch number: 6591 and the loss: 34287.48828125\n",
      "Epoch number: 6601 and the loss: 33088.51171875\n",
      "Epoch number: 6611 and the loss: 34473.3203125\n",
      "Epoch number: 6621 and the loss: 35298.9609375\n",
      "Epoch number: 6631 and the loss: 33110.18359375\n",
      "Epoch number: 6641 and the loss: 35575.28125\n",
      "Epoch number: 6651 and the loss: 33531.7578125\n",
      "Epoch number: 6661 and the loss: 35103.94921875\n",
      "Epoch number: 6671 and the loss: 34981.4765625\n",
      "Epoch number: 6681 and the loss: 34460.32421875\n",
      "Epoch number: 6691 and the loss: 34088.33203125\n",
      "Epoch number: 6701 and the loss: 36114.27734375\n",
      "Epoch number: 6711 and the loss: 32615.876953125\n",
      "Epoch number: 6721 and the loss: 33707.94921875\n",
      "Epoch number: 6731 and the loss: 34322.765625\n",
      "Epoch number: 6741 and the loss: 34672.046875\n",
      "Epoch number: 6751 and the loss: 32479.236328125\n",
      "Epoch number: 6761 and the loss: 33920.9453125\n",
      "Epoch number: 6771 and the loss: 35093.90234375\n",
      "Epoch number: 6781 and the loss: 33726.296875\n",
      "Epoch number: 6791 and the loss: 34147.0859375\n",
      "Epoch number: 6801 and the loss: 33941.17578125\n",
      "Epoch number: 6811 and the loss: 35005.71484375\n",
      "Epoch number: 6821 and the loss: 35326.12109375\n",
      "Epoch number: 6831 and the loss: 33806.8359375\n",
      "Epoch number: 6841 and the loss: 34054.35546875\n",
      "Epoch number: 6851 and the loss: 32371.48046875\n",
      "Epoch number: 6861 and the loss: 34593.90625\n",
      "Epoch number: 6871 and the loss: 32306.59375\n",
      "Epoch number: 6881 and the loss: 34917.88671875\n",
      "Epoch number: 6891 and the loss: 35001.12109375\n",
      "Epoch number: 6901 and the loss: 34169.61328125\n",
      "Epoch number: 6911 and the loss: 34436.3984375\n",
      "Epoch number: 6921 and the loss: 39635.7890625\n",
      "Epoch number: 6931 and the loss: 34772.03515625\n",
      "Epoch number: 6941 and the loss: 35113.23828125\n",
      "Epoch number: 6951 and the loss: 34738.4921875\n",
      "Epoch number: 6961 and the loss: 33631.09375\n",
      "Epoch number: 6971 and the loss: 34650.0\n",
      "Epoch number: 6981 and the loss: 33227.68359375\n",
      "Epoch number: 6991 and the loss: 35213.8828125\n",
      "Epoch number: 7001 and the loss: 33847.5390625\n",
      "Epoch number: 7011 and the loss: 34571.421875\n",
      "Epoch number: 7021 and the loss: 35212.35546875\n",
      "Epoch number: 7031 and the loss: 34921.5625\n",
      "Epoch number: 7041 and the loss: 34145.73046875\n",
      "Epoch number: 7051 and the loss: 35036.625\n",
      "Epoch number: 7061 and the loss: 33560.8203125\n",
      "Epoch number: 7071 and the loss: 34113.359375\n",
      "Epoch number: 7081 and the loss: 33804.734375\n",
      "Epoch number: 7091 and the loss: 33488.984375\n",
      "Epoch number: 7101 and the loss: 33404.0546875\n",
      "Epoch number: 7111 and the loss: 33447.0\n",
      "Epoch number: 7121 and the loss: 33977.984375\n",
      "Epoch number: 7131 and the loss: 34660.40625\n",
      "Epoch number: 7141 and the loss: 34442.421875\n",
      "Epoch number: 7151 and the loss: 34339.79296875\n",
      "Epoch number: 7161 and the loss: 34676.90234375\n",
      "Epoch number: 7171 and the loss: 34686.18359375\n",
      "Epoch number: 7181 and the loss: 34977.91015625\n",
      "Epoch number: 7191 and the loss: 35049.96484375\n",
      "Epoch number: 7201 and the loss: 34424.54296875\n",
      "Epoch number: 7211 and the loss: 34282.8125\n",
      "Epoch number: 7221 and the loss: 34982.16796875\n",
      "Epoch number: 7231 and the loss: 34489.703125\n",
      "Epoch number: 7241 and the loss: 34749.70703125\n",
      "Epoch number: 7251 and the loss: 35453.734375\n",
      "Epoch number: 7261 and the loss: 33480.234375\n",
      "Epoch number: 7271 and the loss: 34005.63671875\n",
      "Epoch number: 7281 and the loss: 33810.3046875\n",
      "Epoch number: 7291 and the loss: 34867.7734375\n",
      "Epoch number: 7301 and the loss: 34201.9140625\n",
      "Epoch number: 7311 and the loss: 33724.05078125\n",
      "Epoch number: 7321 and the loss: 33683.21484375\n",
      "Epoch number: 7331 and the loss: 33641.73046875\n",
      "Epoch number: 7341 and the loss: 35613.9296875\n",
      "Epoch number: 7351 and the loss: 33734.36328125\n",
      "Epoch number: 7361 and the loss: 33612.98828125\n",
      "Epoch number: 7371 and the loss: 33320.71875\n",
      "Epoch number: 7381 and the loss: 34060.98828125\n",
      "Epoch number: 7391 and the loss: 35134.7421875\n",
      "Epoch number: 7401 and the loss: 33403.09765625\n",
      "Epoch number: 7411 and the loss: 33678.76171875\n",
      "Epoch number: 7421 and the loss: 35143.875\n",
      "Epoch number: 7431 and the loss: 35352.56640625\n",
      "Epoch number: 7441 and the loss: 34154.8203125\n",
      "Epoch number: 7451 and the loss: 34247.04296875\n",
      "Epoch number: 7461 and the loss: 34896.40625\n",
      "Epoch number: 7471 and the loss: 33601.57421875\n",
      "Epoch number: 7481 and the loss: 34605.59375\n",
      "Epoch number: 7491 and the loss: 34126.18359375\n",
      "Epoch number: 7501 and the loss: 32937.96875\n",
      "Epoch number: 7511 and the loss: 34321.6015625\n",
      "Epoch number: 7521 and the loss: 34020.71875\n",
      "Epoch number: 7531 and the loss: 33141.51953125\n",
      "Epoch number: 7541 and the loss: 35718.87109375\n",
      "Epoch number: 7551 and the loss: 34140.9453125\n",
      "Epoch number: 7561 and the loss: 34411.29296875\n",
      "Epoch number: 7571 and the loss: 32947.94140625\n",
      "Epoch number: 7581 and the loss: 33347.6015625\n",
      "Epoch number: 7591 and the loss: 33789.21484375\n",
      "Epoch number: 7601 and the loss: 34149.4921875\n",
      "Epoch number: 7611 and the loss: 33413.22265625\n",
      "Epoch number: 7621 and the loss: 33487.625\n",
      "Epoch number: 7631 and the loss: 31816.40625\n",
      "Epoch number: 7641 and the loss: 34366.03515625\n",
      "Epoch number: 7651 and the loss: 32844.08984375\n",
      "Epoch number: 7661 and the loss: 34739.08203125\n",
      "Epoch number: 7671 and the loss: 33965.65234375\n",
      "Epoch number: 7681 and the loss: 33061.765625\n",
      "Epoch number: 7691 and the loss: 37555.703125\n",
      "Epoch number: 7701 and the loss: 35053.94140625\n",
      "Epoch number: 7711 and the loss: 34010.7421875\n",
      "Epoch number: 7721 and the loss: 33575.19140625\n",
      "Epoch number: 7731 and the loss: 33436.62109375\n",
      "Epoch number: 7741 and the loss: 33440.16015625\n",
      "Epoch number: 7751 and the loss: 35104.85546875\n",
      "Epoch number: 7761 and the loss: 33263.296875\n",
      "Epoch number: 7771 and the loss: 35462.6171875\n",
      "Epoch number: 7781 and the loss: 35207.3046875\n",
      "Epoch number: 7791 and the loss: 33438.9765625\n",
      "Epoch number: 7801 and the loss: 34976.34375\n",
      "Epoch number: 7811 and the loss: 34276.99609375\n",
      "Epoch number: 7821 and the loss: 34286.81640625\n",
      "Epoch number: 7831 and the loss: 36153.4296875\n",
      "Epoch number: 7841 and the loss: 34989.40625\n",
      "Epoch number: 7851 and the loss: 34680.4140625\n",
      "Epoch number: 7861 and the loss: 32603.794921875\n",
      "Epoch number: 7871 and the loss: 33524.890625\n",
      "Epoch number: 7881 and the loss: 36670.79296875\n",
      "Epoch number: 7891 and the loss: 33842.41015625\n",
      "Epoch number: 7901 and the loss: 34957.625\n",
      "Epoch number: 7911 and the loss: 34028.92578125\n",
      "Epoch number: 7921 and the loss: 34060.2734375\n",
      "Epoch number: 7931 and the loss: 32376.828125\n",
      "Epoch number: 7941 and the loss: 33213.171875\n",
      "Epoch number: 7951 and the loss: 33708.140625\n",
      "Epoch number: 7961 and the loss: 33966.7890625\n",
      "Epoch number: 7971 and the loss: 34436.80078125\n",
      "Epoch number: 7981 and the loss: 32747.173828125\n",
      "Epoch number: 7991 and the loss: 35989.03125\n",
      "Epoch number: 8001 and the loss: 35254.48828125\n",
      "Epoch number: 8011 and the loss: 32513.013671875\n",
      "Epoch number: 8021 and the loss: 34900.91796875\n",
      "Epoch number: 8031 and the loss: 33692.9609375\n",
      "Epoch number: 8041 and the loss: 33424.671875\n",
      "Epoch number: 8051 and the loss: 32814.9140625\n",
      "Epoch number: 8061 and the loss: 34445.03515625\n",
      "Epoch number: 8071 and the loss: 34500.9296875\n",
      "Epoch number: 8081 and the loss: 42129.04296875\n",
      "Epoch number: 8091 and the loss: 34571.21484375\n",
      "Epoch number: 8101 and the loss: 33351.59375\n",
      "Epoch number: 8111 and the loss: 32910.03515625\n",
      "Epoch number: 8121 and the loss: 33104.41015625\n",
      "Epoch number: 8131 and the loss: 34270.125\n",
      "Epoch number: 8141 and the loss: 34523.19921875\n",
      "Epoch number: 8151 and the loss: 34040.88671875\n",
      "Epoch number: 8161 and the loss: 33205.5390625\n",
      "Epoch number: 8171 and the loss: 35475.75390625\n",
      "Epoch number: 8181 and the loss: 33915.890625\n",
      "Epoch number: 8191 and the loss: 34475.9296875\n",
      "Epoch number: 8201 and the loss: 34915.70703125\n",
      "Epoch number: 8211 and the loss: 34179.94140625\n",
      "Epoch number: 8221 and the loss: 33624.015625\n",
      "Epoch number: 8231 and the loss: 32736.6328125\n",
      "Epoch number: 8241 and the loss: 33105.875\n",
      "Epoch number: 8251 and the loss: 34510.609375\n",
      "Epoch number: 8261 and the loss: 32597.41796875\n",
      "Epoch number: 8271 and the loss: 33504.62890625\n",
      "Epoch number: 8281 and the loss: 35339.34765625\n",
      "Epoch number: 8291 and the loss: 34433.4765625\n",
      "Epoch number: 8301 and the loss: 33363.4375\n",
      "Epoch number: 8311 and the loss: 34474.23828125\n",
      "Epoch number: 8321 and the loss: 34278.11328125\n",
      "Epoch number: 8331 and the loss: 33218.71484375\n",
      "Epoch number: 8341 and the loss: 34231.1015625\n",
      "Epoch number: 8351 and the loss: 34036.14453125\n",
      "Epoch number: 8361 and the loss: 33804.85546875\n",
      "Epoch number: 8371 and the loss: 33138.08984375\n",
      "Epoch number: 8381 and the loss: 34250.30078125\n",
      "Epoch number: 8391 and the loss: 33975.29296875\n",
      "Epoch number: 8401 and the loss: 34133.1796875\n",
      "Epoch number: 8411 and the loss: 33483.94921875\n",
      "Epoch number: 8421 and the loss: 34524.25390625\n",
      "Epoch number: 8431 and the loss: 33706.51953125\n",
      "Epoch number: 8441 and the loss: 36293.484375\n",
      "Epoch number: 8451 and the loss: 33832.20703125\n",
      "Epoch number: 8461 and the loss: 34189.65234375\n",
      "Epoch number: 8471 and the loss: 33885.53515625\n",
      "Epoch number: 8481 and the loss: 34256.8203125\n",
      "Epoch number: 8491 and the loss: 32595.517578125\n",
      "Epoch number: 8501 and the loss: 33788.35546875\n",
      "Epoch number: 8511 and the loss: 35606.4140625\n",
      "Epoch number: 8521 and the loss: 34059.24609375\n",
      "Epoch number: 8531 and the loss: 33746.7265625\n",
      "Epoch number: 8541 and the loss: 34662.375\n",
      "Epoch number: 8551 and the loss: 35674.67578125\n",
      "Epoch number: 8561 and the loss: 33474.96484375\n",
      "Epoch number: 8571 and the loss: 34077.0546875\n",
      "Epoch number: 8581 and the loss: 34454.6875\n",
      "Epoch number: 8591 and the loss: 34251.75390625\n",
      "Epoch number: 8601 and the loss: 33931.359375\n",
      "Epoch number: 8611 and the loss: 33875.23828125\n",
      "Epoch number: 8621 and the loss: 33901.21875\n",
      "Epoch number: 8631 and the loss: 33771.109375\n",
      "Epoch number: 8641 and the loss: 35135.50390625\n",
      "Epoch number: 8651 and the loss: 35893.94921875\n",
      "Epoch number: 8661 and the loss: 34115.34375\n",
      "Epoch number: 8671 and the loss: 34067.62890625\n",
      "Epoch number: 8681 and the loss: 33429.64453125\n",
      "Epoch number: 8691 and the loss: 34596.8203125\n",
      "Epoch number: 8701 and the loss: 33811.86328125\n",
      "Epoch number: 8711 and the loss: 34006.26171875\n",
      "Epoch number: 8721 and the loss: 34218.3828125\n",
      "Epoch number: 8731 and the loss: 36359.08984375\n",
      "Epoch number: 8741 and the loss: 32813.7578125\n",
      "Epoch number: 8751 and the loss: 33183.95703125\n",
      "Epoch number: 8761 and the loss: 33719.97265625\n",
      "Epoch number: 8771 and the loss: 34964.4453125\n",
      "Epoch number: 8781 and the loss: 31714.384765625\n",
      "Epoch number: 8791 and the loss: 33524.0390625\n",
      "Epoch number: 8801 and the loss: 34485.6796875\n",
      "Epoch number: 8811 and the loss: 35224.06640625\n",
      "Epoch number: 8821 and the loss: 37209.13671875\n",
      "Epoch number: 8831 and the loss: 32268.869140625\n",
      "Epoch number: 8841 and the loss: 35576.890625\n",
      "Epoch number: 8851 and the loss: 33398.125\n",
      "Epoch number: 8861 and the loss: 34674.8828125\n",
      "Epoch number: 8871 and the loss: 33617.76953125\n",
      "Epoch number: 8881 and the loss: 33300.22265625\n",
      "Epoch number: 8891 and the loss: 34281.765625\n",
      "Epoch number: 8901 and the loss: 35029.60546875\n",
      "Epoch number: 8911 and the loss: 33099.96484375\n",
      "Epoch number: 8921 and the loss: 36246.3515625\n",
      "Epoch number: 8931 and the loss: 33362.484375\n",
      "Epoch number: 8941 and the loss: 33772.390625\n",
      "Epoch number: 8951 and the loss: 33093.89453125\n",
      "Epoch number: 8961 and the loss: 33795.33203125\n",
      "Epoch number: 8971 and the loss: 32846.46484375\n",
      "Epoch number: 8981 and the loss: 33918.171875\n",
      "Epoch number: 8991 and the loss: 34670.76171875\n",
      "Epoch number: 9001 and the loss: 31549.943359375\n",
      "Epoch number: 9011 and the loss: 36986.65625\n",
      "Epoch number: 9021 and the loss: 33529.34765625\n",
      "Epoch number: 9031 and the loss: 35274.74609375\n",
      "Epoch number: 9041 and the loss: 35752.6953125\n",
      "Epoch number: 9051 and the loss: 32801.16015625\n",
      "Epoch number: 9061 and the loss: 34146.9296875\n",
      "Epoch number: 9071 and the loss: 33694.234375\n",
      "Epoch number: 9081 and the loss: 34110.45703125\n",
      "Epoch number: 9091 and the loss: 34385.7421875\n",
      "Epoch number: 9101 and the loss: 32647.3125\n",
      "Epoch number: 9111 and the loss: 32740.701171875\n",
      "Epoch number: 9121 and the loss: 32568.443359375\n",
      "Epoch number: 9131 and the loss: 35287.671875\n",
      "Epoch number: 9141 and the loss: 33484.80859375\n",
      "Epoch number: 9151 and the loss: 34424.15234375\n",
      "Epoch number: 9161 and the loss: 33672.99609375\n",
      "Epoch number: 9171 and the loss: 35040.8671875\n",
      "Epoch number: 9181 and the loss: 33139.546875\n",
      "Epoch number: 9191 and the loss: 33708.32421875\n",
      "Epoch number: 9201 and the loss: 34173.9375\n",
      "Epoch number: 9211 and the loss: 34669.84375\n",
      "Epoch number: 9221 and the loss: 34383.203125\n",
      "Epoch number: 9231 and the loss: 33694.55078125\n",
      "Epoch number: 9241 and the loss: 34225.7578125\n",
      "Epoch number: 9251 and the loss: 34086.47265625\n",
      "Epoch number: 9261 and the loss: 34254.5078125\n",
      "Epoch number: 9271 and the loss: 34070.40625\n",
      "Epoch number: 9281 and the loss: 40566.1953125\n",
      "Epoch number: 9291 and the loss: 33747.76953125\n",
      "Epoch number: 9301 and the loss: 32855.6953125\n",
      "Epoch number: 9311 and the loss: 32771.92578125\n",
      "Epoch number: 9321 and the loss: 33823.125\n",
      "Epoch number: 9331 and the loss: 32626.671875\n",
      "Epoch number: 9341 and the loss: 34398.4453125\n",
      "Epoch number: 9351 and the loss: 33000.0390625\n",
      "Epoch number: 9361 and the loss: 33030.265625\n",
      "Epoch number: 9371 and the loss: 33196.73828125\n",
      "Epoch number: 9381 and the loss: 34363.4140625\n",
      "Epoch number: 9391 and the loss: 33395.13671875\n",
      "Epoch number: 9401 and the loss: 33021.453125\n",
      "Epoch number: 9411 and the loss: 33269.359375\n",
      "Epoch number: 9421 and the loss: 35416.84375\n",
      "Epoch number: 9431 and the loss: 32357.837890625\n",
      "Epoch number: 9441 and the loss: 33285.96875\n",
      "Epoch number: 9451 and the loss: 33508.4375\n",
      "Epoch number: 9461 and the loss: 32246.673828125\n",
      "Epoch number: 9471 and the loss: 34333.51953125\n",
      "Epoch number: 9481 and the loss: 34022.69921875\n",
      "Epoch number: 9491 and the loss: 33397.5546875\n",
      "Epoch number: 9501 and the loss: 33997.18359375\n",
      "Epoch number: 9511 and the loss: 33438.52734375\n",
      "Epoch number: 9521 and the loss: 34213.94140625\n",
      "Epoch number: 9531 and the loss: 33419.87109375\n",
      "Epoch number: 9541 and the loss: 33163.38671875\n",
      "Epoch number: 9551 and the loss: 33676.75\n",
      "Epoch number: 9561 and the loss: 33678.8125\n",
      "Epoch number: 9571 and the loss: 34491.3203125\n",
      "Epoch number: 9581 and the loss: 34494.47265625\n",
      "Epoch number: 9591 and the loss: 33826.12109375\n",
      "Epoch number: 9601 and the loss: 44573.8828125\n",
      "Epoch number: 9611 and the loss: 34035.96875\n",
      "Epoch number: 9621 and the loss: 34842.85546875\n",
      "Epoch number: 9631 and the loss: 34087.1796875\n",
      "Epoch number: 9641 and the loss: 35200.02734375\n",
      "Epoch number: 9651 and the loss: 33434.18359375\n",
      "Epoch number: 9661 and the loss: 34192.2265625\n",
      "Epoch number: 9671 and the loss: 33298.80078125\n",
      "Epoch number: 9681 and the loss: 31328.5234375\n",
      "Epoch number: 9691 and the loss: 34286.46484375\n",
      "Epoch number: 9701 and the loss: 34996.13671875\n",
      "Epoch number: 9711 and the loss: 31809.7265625\n",
      "Epoch number: 9721 and the loss: 32701.041015625\n",
      "Epoch number: 9731 and the loss: 32361.595703125\n",
      "Epoch number: 9741 and the loss: 35514.80078125\n",
      "Epoch number: 9751 and the loss: 32416.517578125\n",
      "Epoch number: 9761 and the loss: 32560.767578125\n",
      "Epoch number: 9771 and the loss: 34582.84375\n",
      "Epoch number: 9781 and the loss: 33506.94140625\n",
      "Epoch number: 9791 and the loss: 34245.6953125\n",
      "Epoch number: 9801 and the loss: 33055.5\n",
      "Epoch number: 9811 and the loss: 33968.32421875\n",
      "Epoch number: 9821 and the loss: 33472.33984375\n",
      "Epoch number: 9831 and the loss: 34725.9296875\n",
      "Epoch number: 9841 and the loss: 33246.3515625\n",
      "Epoch number: 9851 and the loss: 33843.50390625\n",
      "Epoch number: 9861 and the loss: 35098.50390625\n",
      "Epoch number: 9871 and the loss: 36464.5703125\n",
      "Epoch number: 9881 and the loss: 32790.5390625\n",
      "Epoch number: 9891 and the loss: 33397.33203125\n",
      "Epoch number: 9901 and the loss: 33656.1796875\n",
      "Epoch number: 9911 and the loss: 34608.7109375\n",
      "Epoch number: 9921 and the loss: 33902.74609375\n",
      "Epoch number: 9931 and the loss: 35911.25\n",
      "Epoch number: 9941 and the loss: 33775.40234375\n",
      "Epoch number: 9951 and the loss: 34591.109375\n",
      "Epoch number: 9961 and the loss: 33389.09765625\n",
      "Epoch number: 9971 and the loss: 34408.05859375\n",
      "Epoch number: 9981 and the loss: 32163.525390625\n",
      "Epoch number: 9991 and the loss: 32922.57421875\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "final_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i = i + 1\n",
    "    y_pred = model(train_categorical, train_cont)\n",
    "    loss = torch.sqrt(loss_function(y_pred,y_train)) ## RMSE loss\n",
    "\n",
    "    final_losses.append(loss)\n",
    "    if i%10 == 1:\n",
    "        print(f\"Epoch number: {i} and the loss: {loss.item()}\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() ### backpropagation\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'RMSE Loss')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(epochs), final_losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"RMSE Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE loss: 46738.1015625\n"
     ]
    }
   ],
   "source": [
    "# Validate teh test data\n",
    "y_pred = \"\"\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_categorical, test_cont)\n",
    "    loss = torch.sqrt(loss_function(y_pred,y_test))\n",
    "\n",
    "print(f\"RMSE loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_verify = pd.DataFrame(y_test.tolist(), columns=['Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predicted = pd.DataFrame(y_pred.tolist(), columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159069.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184955.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192130.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>305576.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>167255.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>189193.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>190151.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>177161.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>104445.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>352079.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>215091.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>145684.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>142971.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>95945.273438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>212200.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>102520.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>215597.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>133757.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>303198.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>69514.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>176602.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>126235.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>126551.054688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>232423.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>150946.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>148303.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>141836.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>394015.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>148625.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>182318.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>69413.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>129001.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>317137.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>114366.070312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>229463.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>158107.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>116131.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>155330.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>167320.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>108900.554688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>234869.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>270906.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>88959.789062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>131132.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>200602.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>83098.851562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>128305.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>125279.226562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>128969.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>387672.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>139018.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>297077.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>216099.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>164280.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>162615.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>252809.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>128976.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>104057.351562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>207234.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>201726.265625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>370798.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>101904.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>223846.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>248179.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>161956.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>102942.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>158641.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>221111.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>72903.570312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>270656.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>64536.605469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>94136.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>98213.523438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>224797.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>186543.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>253824.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>127716.429688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>108787.804688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>90882.398438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>107904.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>156296.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>170051.671875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83099.945312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>236970.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>105214.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>111163.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>152845.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>162276.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>176682.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>53257.082031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>297121.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>173572.453125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>193484.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>187739.265625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>114744.632812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>332645.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>206949.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>316642.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>311843.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>267077.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>193226.265625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>129687.226562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>227015.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>102927.117188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>268009.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>114715.851562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>141662.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>311378.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>258336.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>241620.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>91234.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>178575.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>60263.050781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>168420.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>77213.289062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>118332.664062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>134154.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>106332.460938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>373629.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>148806.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>263898.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>93767.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>195229.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>150200.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>96800.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>122767.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>170636.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>368227.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>130825.945312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>190648.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>116822.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>102736.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>206259.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>203724.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>290316.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>116327.945312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>193721.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>128237.148438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>90369.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>328574.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>252827.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>156452.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>85684.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>352531.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>165556.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>215677.453125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>99146.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>134028.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>170930.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>123232.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>135542.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>186864.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>309220.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>116610.648438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>84032.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>198496.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>105182.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>236537.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>144161.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>144654.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>97609.421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>320355.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>114239.023438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>207456.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>263723.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>281143.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>182582.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>108833.382812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>316618.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>107504.351562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>60769.527344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>146177.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>253778.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>153275.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>173001.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>148035.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>226151.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>184431.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>249414.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>105221.320312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Prediction\n",
       "0    159069.000000\n",
       "1    184955.281250\n",
       "2    192130.593750\n",
       "3    305576.593750\n",
       "4    167255.281250\n",
       "5    189193.218750\n",
       "6    190151.187500\n",
       "7    177161.859375\n",
       "8    104445.703125\n",
       "9    352079.375000\n",
       "10   215091.703125\n",
       "11   145684.687500\n",
       "12   142971.734375\n",
       "13    95945.273438\n",
       "14   212200.859375\n",
       "15   102520.679688\n",
       "16   215597.718750\n",
       "17   133757.468750\n",
       "18   303198.656250\n",
       "19    69514.218750\n",
       "20   176602.078125\n",
       "21   126235.921875\n",
       "22   126551.054688\n",
       "23   232423.937500\n",
       "24   150946.062500\n",
       "25   148303.531250\n",
       "26   141836.984375\n",
       "27   394015.343750\n",
       "28   148625.578125\n",
       "29   182318.171875\n",
       "30    69413.375000\n",
       "31   129001.312500\n",
       "32   317137.968750\n",
       "33   114366.070312\n",
       "34   229463.562500\n",
       "35   158107.234375\n",
       "36   116131.171875\n",
       "37   155330.093750\n",
       "38   167320.437500\n",
       "39   108900.554688\n",
       "40   234869.000000\n",
       "41   270906.531250\n",
       "42    88959.789062\n",
       "43   131132.140625\n",
       "44   200602.781250\n",
       "45    83098.851562\n",
       "46   128305.914062\n",
       "47   125279.226562\n",
       "48   128969.218750\n",
       "49   387672.062500\n",
       "50   139018.906250\n",
       "51   297077.343750\n",
       "52   216099.703125\n",
       "53   164280.500000\n",
       "54   162615.562500\n",
       "55   252809.562500\n",
       "56   128976.406250\n",
       "57   104057.351562\n",
       "58   207234.718750\n",
       "59   201726.265625\n",
       "60   370798.062500\n",
       "61   101904.085938\n",
       "62   223846.156250\n",
       "63   248179.171875\n",
       "64   161956.406250\n",
       "65   102942.898438\n",
       "66   158641.375000\n",
       "67   221111.281250\n",
       "68    72903.570312\n",
       "69   270656.687500\n",
       "70    64536.605469\n",
       "71    94136.406250\n",
       "72    98213.523438\n",
       "73   224797.703125\n",
       "74   186543.921875\n",
       "75   253824.343750\n",
       "76   127716.429688\n",
       "77   108787.804688\n",
       "78    90882.398438\n",
       "79   107904.085938\n",
       "80   156296.796875\n",
       "81   170051.671875\n",
       "82    83099.945312\n",
       "83   236970.578125\n",
       "84   105214.765625\n",
       "85   111163.914062\n",
       "86   152845.937500\n",
       "87   162276.796875\n",
       "88   176682.343750\n",
       "89    53257.082031\n",
       "90   297121.031250\n",
       "91   173572.453125\n",
       "92   193484.281250\n",
       "93   187739.265625\n",
       "94   114744.632812\n",
       "95   332645.531250\n",
       "96   206949.093750\n",
       "97   316642.156250\n",
       "98   311843.093750\n",
       "99   267077.625000\n",
       "100  193226.265625\n",
       "101  129687.226562\n",
       "102  227015.328125\n",
       "103  102927.117188\n",
       "104  268009.625000\n",
       "105  114715.851562\n",
       "106  141662.812500\n",
       "107  311378.781250\n",
       "108  258336.375000\n",
       "109  241620.062500\n",
       "110   91234.562500\n",
       "111  178575.765625\n",
       "112   60263.050781\n",
       "113  168420.812500\n",
       "114   77213.289062\n",
       "115  118332.664062\n",
       "116  134154.234375\n",
       "117  106332.460938\n",
       "118  373629.812500\n",
       "119  148806.437500\n",
       "120  263898.718750\n",
       "121   93767.710938\n",
       "122  195229.703125\n",
       "123  150200.500000\n",
       "124   96800.312500\n",
       "125  122767.937500\n",
       "126  170636.406250\n",
       "127  368227.031250\n",
       "128  130825.945312\n",
       "129  190648.781250\n",
       "130  116822.328125\n",
       "131  102736.085938\n",
       "132  206259.531250\n",
       "133  203724.406250\n",
       "134  290316.718750\n",
       "135  116327.945312\n",
       "136  193721.703125\n",
       "137  128237.148438\n",
       "138   90369.953125\n",
       "139  328574.437500\n",
       "140  252827.500000\n",
       "141  156452.500000\n",
       "142   85684.296875\n",
       "143  352531.218750\n",
       "144  165556.093750\n",
       "145  215677.453125\n",
       "146   99146.007812\n",
       "147  134028.531250\n",
       "148  170930.437500\n",
       "149  123232.218750\n",
       "150  135542.906250\n",
       "151  186864.906250\n",
       "152  309220.062500\n",
       "153  116610.648438\n",
       "154   84032.085938\n",
       "155  198496.625000\n",
       "156  105182.710938\n",
       "157  236537.906250\n",
       "158  144161.468750\n",
       "159  144654.437500\n",
       "160   97609.421875\n",
       "161  320355.750000\n",
       "162  114239.023438\n",
       "163  207456.578125\n",
       "164  263723.218750\n",
       "165  281143.218750\n",
       "166  182582.109375\n",
       "167  108833.382812\n",
       "168  316618.031250\n",
       "169  107504.351562\n",
       "170   60769.527344\n",
       "171  146177.031250\n",
       "172  253778.843750\n",
       "173  153275.187500\n",
       "174  173001.781250\n",
       "175  148035.875000\n",
       "176  226151.906250\n",
       "177  184431.500000\n",
       "178  249414.296875\n",
       "179  105221.320312"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130000.0</td>\n",
       "      <td>159069.00000</td>\n",
       "      <td>-29069.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138887.0</td>\n",
       "      <td>184955.28125</td>\n",
       "      <td>-46068.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175500.0</td>\n",
       "      <td>192130.59375</td>\n",
       "      <td>-16630.59375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195000.0</td>\n",
       "      <td>305576.59375</td>\n",
       "      <td>-110576.59375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142500.0</td>\n",
       "      <td>167255.28125</td>\n",
       "      <td>-24755.28125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Test    Prediction    Difference\n",
       "0  130000.0  159069.00000  -29069.00000\n",
       "1  138887.0  184955.28125  -46068.28125\n",
       "2  175500.0  192130.59375  -16630.59375\n",
       "3  195000.0  305576.59375 -110576.59375\n",
       "4  142500.0  167255.28125  -24755.28125"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output = pd.concat([data_verify,data_predicted], axis=1)\n",
    "final_output['Difference'] = final_output['Test']-final_output['Prediction']\n",
    "\n",
    "final_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model\n",
    "torch.save(model, 'HousePrice.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the weights for the model\n",
    "torch.save(model.state_dict(), 'HouseWeight.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model \n",
    "\n",
    "# The embedding matrix is required for the model\n",
    "embs_size = [(15,8),(5,3),(2,1),(4,2)]\n",
    "\n",
    "model1 = FeedForwardNN(embs_size, 5, 1, [100,50], p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.load_state_dict(torch.load('HouseWeight.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
